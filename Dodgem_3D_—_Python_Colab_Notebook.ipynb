{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZlnVGG9K5or+AxptbtnY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pt-home/Dodgem/blob/main/Dodgem_3D_%E2%80%94_Python_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bc66-Eth9qRG"
      },
      "outputs": [],
      "source": [
        "# üßä Dodgem 3D ‚Äî 3 Players, Q-Learning Simulation\n",
        "\n",
        "import random\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
        "board_size = 3\n",
        "pieces_per_player = board_size - 1\n",
        "games_to_play = 500000\n",
        "learning_rate = 0.1\n",
        "epsilon = 0.1\n",
        "players = ['X', 'Y', 'Z']\n",
        "\n",
        "own_axis = {'X': 0, 'Y': 1, 'Z': 2}\n",
        "non_axes = {'X': [1, 2], 'Y': [0, 2], 'Z': [0, 1]}"
      ],
      "metadata": {
        "id": "-ch8SgIt9ykx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ –ü–æ—á–∞—Ç–∫–æ–≤–∞ –ø–æ–∑–∏—Ü—ñ—è\n",
        "def initial_state():\n",
        "    state = {}\n",
        "    for player in players:\n",
        "        axis = own_axis[player]\n",
        "        pieces = []\n",
        "        for i in range(pieces_per_player):\n",
        "            if player == 'X':\n",
        "                pieces.append((board_size - pieces_per_player + i, 0, 0))\n",
        "            elif player == 'Y':\n",
        "                pieces.append((0, board_size - pieces_per_player + i, 0))\n",
        "            elif player == 'Z':\n",
        "                pieces.append((0, 0, board_size - pieces_per_player + i))\n",
        "        state[player] = pieces\n",
        "    return (state, 'X')\n",
        "\n",
        "# üß† –î–æ–∑–≤–æ–ª–µ–Ω—ñ —Ö–æ–¥–∏\n",
        "def get_legal_moves(state, player):\n",
        "    pos_dict, _ = state\n",
        "    occupied = set(p for pl in pos_dict.values() for p in pl if p != 'out')\n",
        "    legal = []\n",
        "    axis = own_axis[player]\n",
        "    for i, pos in enumerate(pos_dict[player]):\n",
        "        if pos == 'out':\n",
        "            continue\n",
        "        coords = list(pos)\n",
        "        # —Ä—É—Ö –ø–æ —Å–≤–æ—ó–π –æ—Å—ñ ‚Äî –≤—ñ–ª—å–Ω–∏–π\n",
        "        for delta in [-1, 1]:\n",
        "            new_coords = coords.copy()\n",
        "            new_coords[axis] += delta\n",
        "            if 0 <= new_coords[axis] < board_size:\n",
        "                if tuple(new_coords) not in occupied:\n",
        "                    legal.append((i, tuple(new_coords)))\n",
        "        # —Ä—É—Ö –≤–ø–µ—Ä–µ–¥ –ø–æ –Ω–µ–≤–ª–∞—Å–Ω–∏—Ö\n",
        "        for dim in non_axes[player]:\n",
        "            new_coords = coords.copy()\n",
        "            new_coords[dim] += 1\n",
        "            if new_coords[dim] < board_size and tuple(new_coords) not in occupied:\n",
        "                legal.append((i, tuple(new_coords)))\n",
        "        # —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π —Ö—ñ–¥: –≤–∏—Ö—ñ–¥\n",
        "        if all(coords[dim] == board_size - 1 for dim in non_axes[player]):\n",
        "            legal.append((i, 'exit'))\n",
        "    return legal\n",
        "\n",
        "# üö™ –í–∏–∫–æ–Ω–∞–Ω–Ω—è —Ö–æ–¥—É\n",
        "def apply_move(state, move, player):\n",
        "    pos_dict, current = state\n",
        "    new_pos = {p: list(pos_dict[p]) for p in players}\n",
        "    i, target = move\n",
        "    if target == 'exit':\n",
        "        new_pos[player][i] = 'out'\n",
        "    else:\n",
        "        new_pos[player][i] = target\n",
        "    next_player = players[(players.index(player) + 1) % 3]\n",
        "    return (new_pos, next_player)\n",
        "\n",
        "# üèÜ –ü–µ—Ä–µ–º–æ–≥–∞\n",
        "def check_winner(state):\n",
        "    pos_dict, _ = state\n",
        "    for player in players:\n",
        "        if all(p == 'out' for p in pos_dict[player]):\n",
        "            return player\n",
        "    return None"
      ],
      "metadata": {
        "id": "6nRLmAmL97Fw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-—Ç–∞–±–ª–∏—Ü—è\n",
        "q_table = {p: defaultdict(lambda: defaultdict(lambda: 1.0)) for p in players}\n"
      ],
      "metadata": {
        "id": "gkGmIFAC98lg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ –í–∏–±—ñ—Ä —Ö–æ–¥—É\n",
        "def choose_move(player, state):\n",
        "    legal = get_legal_moves(state, player)\n",
        "    if not legal:\n",
        "        return None\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(legal)\n",
        "    weights = q_table[player][str(state)]\n",
        "    weighted = [(m, weights[m]) for m in legal]\n",
        "    total = sum(w for _, w in weighted)\n",
        "    if total <= 0:\n",
        "        return random.choice(legal)\n",
        "    probs = [w / total for _, w in weighted]\n",
        "    return random.choices([m for m, _ in weighted], probs)[0]\n",
        "\n",
        "# üéÆ –ì—Ä–∞\n",
        "def play_game():\n",
        "    state = initial_state()\n",
        "    trajectory = []\n",
        "    while True:\n",
        "        winner = check_winner(state)\n",
        "        if winner:\n",
        "            return winner, trajectory\n",
        "        player = state[1]\n",
        "        move = choose_move(player, state)\n",
        "        if move:\n",
        "            trajectory.append((player, str(state), move))\n",
        "            state = apply_move(state, move, player)\n",
        "        else:\n",
        "            state = (state[0], players[(players.index(player) + 1) % 3])\n",
        "\n",
        "# üß™ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
        "def train(n):\n",
        "    wins = Counter()\n",
        "    for i in range(1, n + 1):\n",
        "        result, path = play_game()\n",
        "        wins[result] += 1\n",
        "        for player, state_str, move in path:\n",
        "            reward = 1 if result == player else 0\n",
        "            q_table[player][state_str][move] += learning_rate * (reward - q_table[player][state_str][move])\n",
        "        if i % 10000 == 0:\n",
        "            total = sum(wins.values())\n",
        "            print(f\"After {i} games:\", ', '.join(\n",
        "                [f\"{p} = {wins[p]} ({100 * wins[p]/total:.1f}%)\" for p in players]))\n",
        "    return wins"
      ],
      "metadata": {
        "id": "qH0VJzw_-GRA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
        "results = train(games_to_play)\n",
        "print(\"‚úÖ Final results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwTeXLVz-Hnv",
        "outputId": "7c03c9fc-9c76-4893-b30c-1baca89cce61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 10000 games: X = 3778 (37.8%), Y = 3352 (33.5%), Z = 2870 (28.7%)\n",
            "After 20000 games: X = 7577 (37.9%), Y = 6600 (33.0%), Z = 5823 (29.1%)\n",
            "After 30000 games: X = 11316 (37.7%), Y = 9902 (33.0%), Z = 8782 (29.3%)\n",
            "After 40000 games: X = 15076 (37.7%), Y = 13239 (33.1%), Z = 11685 (29.2%)\n",
            "After 50000 games: X = 18912 (37.8%), Y = 16550 (33.1%), Z = 14538 (29.1%)\n",
            "After 60000 games: X = 22667 (37.8%), Y = 19938 (33.2%), Z = 17395 (29.0%)\n",
            "After 70000 games: X = 26527 (37.9%), Y = 23273 (33.2%), Z = 20200 (28.9%)\n",
            "After 80000 games: X = 30269 (37.8%), Y = 26608 (33.3%), Z = 23123 (28.9%)\n",
            "After 90000 games: X = 33997 (37.8%), Y = 29953 (33.3%), Z = 26050 (28.9%)\n",
            "After 100000 games: X = 37803 (37.8%), Y = 33289 (33.3%), Z = 28908 (28.9%)\n",
            "After 110000 games: X = 41611 (37.8%), Y = 36586 (33.3%), Z = 31803 (28.9%)\n",
            "After 120000 games: X = 45406 (37.8%), Y = 39954 (33.3%), Z = 34640 (28.9%)\n",
            "After 130000 games: X = 49171 (37.8%), Y = 43251 (33.3%), Z = 37578 (28.9%)\n",
            "After 140000 games: X = 53018 (37.9%), Y = 46543 (33.2%), Z = 40439 (28.9%)\n",
            "After 150000 games: X = 56857 (37.9%), Y = 49718 (33.1%), Z = 43425 (28.9%)\n",
            "After 160000 games: X = 60598 (37.9%), Y = 53129 (33.2%), Z = 46273 (28.9%)\n",
            "After 170000 games: X = 64411 (37.9%), Y = 56379 (33.2%), Z = 49210 (28.9%)\n",
            "After 180000 games: X = 68230 (37.9%), Y = 59706 (33.2%), Z = 52064 (28.9%)\n",
            "After 190000 games: X = 72035 (37.9%), Y = 63018 (33.2%), Z = 54947 (28.9%)\n",
            "After 200000 games: X = 75835 (37.9%), Y = 66359 (33.2%), Z = 57806 (28.9%)\n",
            "After 210000 games: X = 79549 (37.9%), Y = 69723 (33.2%), Z = 60728 (28.9%)\n",
            "After 220000 games: X = 83334 (37.9%), Y = 73088 (33.2%), Z = 63578 (28.9%)\n",
            "After 230000 games: X = 87160 (37.9%), Y = 76426 (33.2%), Z = 66414 (28.9%)\n",
            "After 240000 games: X = 90966 (37.9%), Y = 79730 (33.2%), Z = 69304 (28.9%)\n",
            "After 250000 games: X = 94693 (37.9%), Y = 83025 (33.2%), Z = 72282 (28.9%)\n",
            "After 260000 games: X = 98525 (37.9%), Y = 86343 (33.2%), Z = 75132 (28.9%)\n",
            "After 270000 games: X = 102320 (37.9%), Y = 89664 (33.2%), Z = 78016 (28.9%)\n",
            "After 280000 games: X = 106117 (37.9%), Y = 92910 (33.2%), Z = 80973 (28.9%)\n",
            "After 290000 games: X = 109833 (37.9%), Y = 96151 (33.2%), Z = 84016 (29.0%)\n",
            "After 300000 games: X = 113561 (37.9%), Y = 99501 (33.2%), Z = 86938 (29.0%)\n",
            "After 310000 games: X = 117328 (37.8%), Y = 102873 (33.2%), Z = 89799 (29.0%)\n",
            "After 320000 games: X = 121064 (37.8%), Y = 106296 (33.2%), Z = 92640 (28.9%)\n",
            "After 330000 games: X = 124876 (37.8%), Y = 109618 (33.2%), Z = 95506 (28.9%)\n",
            "After 340000 games: X = 128685 (37.8%), Y = 112898 (33.2%), Z = 98417 (28.9%)\n",
            "After 350000 games: X = 132494 (37.9%), Y = 116256 (33.2%), Z = 101250 (28.9%)\n",
            "After 360000 games: X = 136242 (37.8%), Y = 119536 (33.2%), Z = 104222 (29.0%)\n",
            "After 370000 games: X = 139953 (37.8%), Y = 122873 (33.2%), Z = 107174 (29.0%)\n",
            "After 380000 games: X = 143783 (37.8%), Y = 126130 (33.2%), Z = 110087 (29.0%)\n",
            "After 390000 games: X = 147555 (37.8%), Y = 129416 (33.2%), Z = 113029 (29.0%)\n",
            "After 400000 games: X = 151297 (37.8%), Y = 132695 (33.2%), Z = 116008 (29.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è Q-—Ç–∞–±–ª–∏—Ü—ñ\n",
        "filename = f\"Q3D_Final_{board_size}x{board_size}_G{games_to_play}_lr{learning_rate}_eps{epsilon}.json\"\n",
        "with open(filename, \"w\") as f:\n",
        "    json.dump({p: {s: {str(m): v for m, v in q_table[p][s].items()} for s in q_table[p]} for p in q_table}, f)\n",
        "print(f\"üíæ Q-table saved to: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "55Mqn4mO-eL5",
        "outputId": "3d8b03f6-c833-42cd-ad8a-13bd23b4ff5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'board_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-38f0be0aad57>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è Q-—Ç–∞–±–ª–∏—Ü—ñ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Q3D_{board_size}x{board_size}_G{games_to_play}_lr{learning_rate}_eps{epsilon}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üíæ Saved Q-table to: {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'board_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìà –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞\n",
        "plt.bar(results.keys(), results.values(), color=['red', 'blue', 'green'])\n",
        "plt.title(f\"Dodgem 3D ({board_size}¬≥): Win Distribution\")\n",
        "plt.ylabel(\"Wins\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oSH4db6UWtdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}